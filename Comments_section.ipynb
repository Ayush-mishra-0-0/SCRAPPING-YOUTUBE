{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Comments Scrapping \n",
    "    \n",
    "   **BY - AYUSH KUMAR MISHRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver \n",
    "import urllib\n",
    "import pathlib\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument('--incognito')\n",
    "driver = webdriver.Chrome( options=chrome_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a function for getting the links from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(item):\n",
    "    query = urllib.parse.quote(item)\n",
    "    url = \"https://www.youtube.com/results?search_query=\" + query\n",
    "    driver.get(url)\n",
    "    driver.execute_script(\"window.scrollTo(0, 1000000)\")\n",
    "    driver.maximize_window()\n",
    "    container = driver.find_elements_by_xpath('//*[(@id = \"video-title\")]')\n",
    "    titles = [element.text for element in container] \n",
    "    links = [page.get_attribute(\"href\") for page in container]\n",
    "    return list(zip(titles, links))  \n",
    "dfs = []\n",
    "search_item = [\"Physical Health\",\"Mental health\",\"Social health\",\"Emotional health\",\"Spiritual health\",\"Environmental health\",\"Intellectual health\",\"Occupational health\",\"Financial health\",\"Interpersonal health\",\"Cultural health\",\"Sexual health\",\"Reproductive health\",\"Personal health\",\"Community health\",\"Global health\",\"Public health\",\"Population health\",\"Personal hygiene\",\"Hygiene\",\"Hygiene practices\"] \n",
    "for item in search_item:\n",
    "    data = get_links(item)\n",
    "    df = pd.DataFrame({'Item': [item] * len(data), 'Video Title': [d[0] for d in data], 'Links': [d[1] for d in data]})\n",
    "    dfs.append(df)\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "final_df.to_csv('Links.csv', index=False)\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = final_df.groupby('Item')['Links'].apply(lambda x: x.isnull().sum())\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.dropna(inplace=True)\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the function to scrap the required information from the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_dictionary = {\n",
    "    'Comments': {},\n",
    "    'Video Link': {},\n",
    "    'Video Title': {},\n",
    "    'Item': {}\n",
    "}\n",
    "\n",
    "def scrap(url, i):\n",
    "    print('Fetched date and time - ', datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "    try:\n",
    "        videos_dictionary['Video Link'][i] = url\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating Video Link for index {i}: {e}\")\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(3) \n",
    "    try:\n",
    "        video_title = driver.find_element_by_xpath('//*[@id=\"video-title\"]').text\n",
    "        item = \"Physical Health\" \n",
    "        videos_dictionary['Video Title'][i] = video_title\n",
    "        videos_dictionary['Item'][i] = item\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating Video Title or Item for index {i}: {e}\")\n",
    "    \n",
    "    # Scroll down to load comments\n",
    "    comments = driver.find_element_by_xpath('//*[@id=\"comments\"]')\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", comments)\n",
    "    last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "        new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    username_elems = driver.find_elements_by_xpath('//*[@id=\"author-text\"]')\n",
    "    comment_elems = driver.find_elements_by_xpath('//*[@id=\"content-text\"]')\n",
    "    \n",
    "    comment_string = ''\n",
    "    \n",
    "    for user, comment in zip(username_elems, comment_elems):\n",
    "        comment_string = comment_string + user.text + ' - ' + comment.text + '\\n'\n",
    "    \n",
    "    try:\n",
    "        videos_dictionary['Comments'][i] = comment_string\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating Comments for index {i}: {e}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First check the file exist to save the data that has been scrapped from the links. If the file does not exist, then create one. If it exist then check whether it is empty or not.\n",
    "\n",
    "If not empty, then extract data drom the links not present in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "name = 'Youtube_scrapping_comments.csv'\n",
    "file_name = pathlib.Path(name)\n",
    "if file_name.exists():\n",
    "    print (\"File exist.\")\n",
    "    try :\n",
    "        print(\"Reading the file now\")\n",
    "        df_temp = pd.read_csv(name, index_col = 0)\n",
    "        start = (len(df_temp) )\n",
    "    except:\n",
    "        print(\"File is empty\")\n",
    "else:\n",
    "    print (\"File does not exist\\n\", \"Creating the file\")\n",
    "    file = open(name,\"w+\")\n",
    "    print(\"File created successfully with filename - \",name)\n",
    "print(\"Starting from position - \", start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num in range(start, 309):\n",
    "    url = final_df['Links'].iloc[num]  \n",
    "    print(\"Loop entered\")\n",
    "    try:\n",
    "        print(\"getting link-\", num)\n",
    "        driver.get(url)\n",
    "    except:\n",
    "        print(\"Not getting\")\n",
    "        continue\n",
    "    print(\"=\" * 40) \n",
    "    print(\"Scraping \" + url)\n",
    "    scrap(url, num)\n",
    "    print(\"=\" * 40) \n",
    "print(\"Process ended successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(310, len(final_df)):\n",
    "    url = final_df['Links'].iloc[num]  \n",
    "    print(\"Loop entered\")\n",
    "    try:\n",
    "        print(\"getting link-\", num)\n",
    "        driver.get(url)\n",
    "    except:\n",
    "        print(\"Not getting\")\n",
    "        continue\n",
    "    print(\"=\" * 40) \n",
    "    print(\"Scraping \" + url)\n",
    "    scrap(url, num)\n",
    "    print(\"=\" * 40) \n",
    "print(\"Process ended successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_dict(videos_dictionary)\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data[data['Comments'] != '']\n",
    "data = data[data['Comments'] != '']\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = data.iloc[0]\n",
    "comment = first_row['Cleaned_Comments']\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing URLs\n",
    "import re\n",
    "def remove_URL(text):\n",
    "    return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "data.loc[:, 'Cleaned_Comments'] = data['Comments'].apply(remove_URL)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Comments'], axis = 1, inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I do not prefer to remove the smileys as they also show emotion so instead of removing them we can change to unicode-8 such that out model can also understand our the commenter wants to say."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_special_characters(text):\n",
    "#     emoji_pattern = re.compile(\n",
    "#         '['\n",
    "#         u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "#         u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "#         u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "#         u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "#         u'\\U00002702-\\U000027B0'\n",
    "#         u'\\U000024C2-\\U0001F251'\n",
    "#         ']+',\n",
    "#         flags=re.UNICODE)\n",
    "#     return emoji_pattern.sub(r'', text)\n",
    "# results['Cleaned_Comments'] = results['Cleaned_Comments'].apply(remove_special_characters)\n",
    "# results\n",
    "csv_file_path = 'Youtube_scrapping_comments.csv'\n",
    "data.to_csv(csv_file_path, encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
